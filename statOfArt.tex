\subsection{Segmentation}
%recuperation d'un objet dans l'environnemnt -> interaction utilisateur
%dire que la création d'une base de connaissance est plus utile que pour le corps humain
La première étape lors de ce projet va être de segmenter les images que nous recevons de 
la caméra. Les informations contenues dans une image 3D sont nombreuses et nous devons
déterminer les éléments important pour nos traitements. Dans notre scène,
nous avons besoin des objets proches ou du corps de la personne en face de la caméra, mais 
l'environnement autour des ces objets clés n'est pas important et doit être supprimer pour
gagner du temps lors de nos traitements en supprimant de l'information à traiter.
Une second segmentation est nécessaire pour le traitement du corps humain. Pour cette étape du projet,
nous devons segmenter le corps en plusieurs partie pour pouvoir par la suite les reconnaitres. Si cette
seconde segmentation n'est pas réalisé il ne nous sera pas possible de reconnaître les mains ou encore
la tête si nous ne savons pas délimité les parties du corps.  

\begin{figure}[!h]
  \begin{center}
    \includegraphics[width=8cm]{image/segmentation.png}
    \includegraphics[width=5cm]{image/bodySegmentation.png}
    \caption{Exemple de segmentation recherché pour une pièce intérieur et pour le corps humain}
  \end{center}
\end{figure}
 
\subsubsection{Scène intérieur}
De nombreux travaux ont été réalisé dans la segmentation d'image 2D avant que les caméras 3D ne soit
ouvert au grand public. Les premières méthode de segmentation reposaient sur la détection de contour
comme pour la méthode de P. Arbelaez et al\cite{2DSegmentation1}. Leur méthode repose sur le détecteur
de contour gPb qui est composé de d'un seuillage sur la luminance et sur la couleur et d'une détection
de texture. La fermeture des contours se fait ensuite en utilisant les superpixels. D'autres méthodes
2D utilise un simple seuillage en utilisant par exemple la méthode de N. Otsu\cite{Otsu} pour binariser
l'image et ainsi la segmenter.\\

%peut etre que l on peut rajouter des publi utilisant la depthmap
%voir si on sépare la depthmap et le nuage de point
Avec l'arrivé des caméras 3D de nombreuses recherche ont été effectué sur la segmentation d'image à partir
des information extraite de ce type de caméra. S.A.A Shah et al\cite{3DSegmentation1} utilisent les informations
de l'image de profondeur afin de calculer un vecteur sur chaque pixel. En applicant un seuillage sur la différence
des vecteurs ils obtiennent une segmentation de l'environnement qui leur permet de détecter des objets dans une pièce.
Il est possible à partir de l'image de profondeur de créer un nuage de point, ce qui permet d'obtenir les 
coordonnées 3D des points présents dans l'image de profondeur. Les informations qu'il est possible d'extraire
d'un nuage de point sont différentes et des méthodes de segmentation se sont développé autour de ces informations.\\

T. Rabbani et al\cite{pointCloudSegmentation} utilise les informations obtenus dans un nuage de point afin 
de calculer les normales de chaque point. Ils segmente ensuite l'image en comparant les normales et en appliquant
un seuillage sur cette comparaison. Si l'angle formé par les normales de deux points est super au seuil alors
les points appartiennent à deux régions différentes.\\

Nous pouvons voir que les méthodes cité précédemment sont efficace pour segmenter une scène comportant des objets,
mais elles ne sont pas applicable à un corps humain. Le principal défaut de ces méthodes pour le corps humain est 
que celui-ci est trop lisse. La différence entre les normales ou entre les vecteurs de pixel n'est pas assez important
et est trop instable pour que cela marche sur le corps humain qui peut adopter de nombreuses postures.

\subsubsection{Corps humain}
La segmentation du corps humain est un sujet très complexe, car contrairement au objet celui-ci bouge et adopte
des postures différentes. La méthode la plus souvent utilisé pour résoudre cette problématique est de déterminer
la posture de l'utilisateur et lorsque cette posture est connu il est facile de déterminer les différentes partie
du corps. Ces méthodes nécessitent d'avoir une base de connaissance contenant de nombreuses postures qui doivent
être segmenter et labelisé avec les différentes parties du corps. J. Shotton et al\cite{kinectSegmentation} ont
d'abord créé une base d'apprentissage en calculant un descripteur
et une technique d'apprentissage automatique appelé forêt d'arbres décisionnels\cite{randomDecisionForest}. 
Le descripteur de J. Shotton et al\cite{kinectSegmentation} utilise les informations de l'image de 
profondeur. Afin de déterminer quel posture a l'utilisateur, il utilise une caractèristique reposant sur la valeur
de deux pixels, un pixel x et d'un pixel dont l'offset par rapport au pixel x a été définit.
Lorsque l'utilisateur bouge, le descripteur utilisé précédemment est recalculé sur l'image courante et le résultat 
est comparé au posture de la base d'apprentissage.\\
\begin{figure}[!h]
  \begin{center}
    \includegraphics[width=5cm]{image/shotton.png}
    \caption{Deux exemples de caractèristique du descripteur de J. Shotton et al\cite{kinectSegmentation}. 
    La croix jaune correspond au pixel à classifier et le cercle rouge correspond au pixel décalé.}
    \label{fig:pfhNeighborhood}
  \end{center}
\end{figure}
%La posture ayant une valeur proche du résultat calculé précédemment est la posture de l'utilisateur. 
%De nombreuses méthode utilise cette solution, mais elle modifie le descripteur
%utilisé et la technique d'apprentissage automatique comme nous le verrons dans la suite de ce rapport.

Comme pour la méthode précédente B. Yoo et al\cite{RDB} utilisent les images de profondeur pour déterminer
la posture de l'utilisateur. Cependant, leur descripteur repose sur des caractèristiques plus complexe comme
l'élongation 3D de la forme du corps, le centre de gravité, la rectangularité de la forme ou encore la 
dissymétrie. Grâce à ces caractèristiques ils forment un descripteur qu'il passe dans leur propre outils
d'apprentissage automatique appelé \og Randomized Decision Bush \fg.\\

Y. Liu et al\cite{GIF} ont développé un autre descripteur spécifique à la reconnaissance de la posture du corps
humain appelé \og Geodesic Invariant Feature \fg(GIF). 
Ce descripteur se base sur le calcul de la distance géodésique. La distance géodésique est la distance
entre deux points d'un modèle 3D en ne passant que par les arrête de ce modèle. Etant donné que ce n'est pas un
modèle 3D que fournit la Kinect, mais un nuage de point, les auteurs forment un maillage en créant n arrêtes avec
les n points les plus proches pour un point donné. Cette distance géodésique permet de connaitre l'orientation
des points. Cette orientation est appliqué sur d'autres caratéristiques sensible à la rotation, ce qui permet
à ces caractéristique d'être invariant en rotation. 

\begin{figure}[!h]
  \begin{center}
    \includegraphics[width=10cm]{image/geodesic.png}
    \caption{Distance géodésique d'un corps humain}
  \end{center}
\end{figure}

\subsection{Reconnaissance d'objets}
La reconnaissance d'objet est un sujet assez vaste dans le monde de l'imagerie et il existe de nombreuses
méthodes dans le domaine que ce soit pour des images 2D ou 3D. Dans le cas d'image 3D la méthode la plus utilisé
est le calcul de descripteur, ce qui correspond à un ensemble de caractèristiques représentant un objet spécifique.

\subsubsection{Descripteur}
\label{descriptor}
Le nombre de descripteur qui existe dans le domaine de l'image 3D est assez important c'est pourquoi pour ce rapport,
nous allons nous contenter de décrire seulement les plus utilisées. Le descripteur D2\cite{D2} est un des outils de 
comparaison de forme 3D les plus simple à réaliser. Il se repose sur le calcul de la distance euclidienne entre 
chaque point du modèle 3D. L'ensemble de ces distances permet de créer un histogramme 1D et de comparer ces histogramme
afin de reconnaître un objet. Ce descripteur fournit de bon résultat lorsque les objets à reconnaître sont très 
différents.\\

Le descripteur PFH\cite{PFH} (Point Feature Histograms) est un outil permettant de calculer la courbure moyenne d'un voisinage de point en utilisant un histogramme multi-dimensionnel. Le calcul de la courbure et le fait que ce soit une généralisation permet d'être invariant 
en translation et en rotation, et permet également d'être moins sensible au bruit présent dans le nuage de point. Le voisinage 
dépend de la distance des points avec le point centrale et il ne peut exceder un certain nombre de voisin 
(voir Fig. \ref{fig:pfhNeighborhood}).\\

\begin{figure}[!h]
  \begin{center}
    \includegraphics[width=6cm]{image/PFH.png}
    \caption{Exemple de voisinage pris en compte dans le calcul de la courbure du descripteur PFH}
    \label{fig:pfhNeighborhood}
  \end{center}
\end{figure}

Le descripteur PFH calculé en un point correspond à la relation que ce point a avec l'ensemble des points de son voisinage. Cette relation
est la différence des normals entre deux points. Chaque classe de l'histogramme est composé de l'ensemble des points du voisinage dont 
la relation avec le point centrale est similaire. Une version amélioré du descripteur a été proposé par R. B. Rusu et al\cite{FPFH} appelé
FPFH (Fast Point Feature Histograms). Cette version est plus rapide, car elle calcul un descripteur PFH simplifié, puis elle construit
de nouveaux histogrammes à partir des histogramme simplifié précédent (voir Fig. \ref{fig:fpfhNeighborhood}).\\

\begin{figure}[!h]
  \begin{center}
    \includegraphics[width=7cm]{image/FPFH.png}
    \caption{Exemple de voisinage pris en compte dans le calcul de la courbure du descripteur FPFH}
    \label{fig:fpfhNeighborhood}
  \end{center}
\end{figure}

D. G. Lowe\cite{SIFT} créa un descripteur appelé \og scale-invariant feature transform \fg(SIFT) qui crée
des points clés dans une image 2D et calcul des descripteurs sur ces points. Ce descripteur permet entre autre
de détecter des points clés similaires dans deux images différentes d'une même scène tant que les angles de vue
sont suffisament petit. Les points clés de ce descripteur sont des zones circulaires positionné sur les extrema 
dans l'espace des échelles, le facteur d'échelle étant proportionnel à la taille de la zone d'intérêt. Une fois que
que l'on a trouvé la position des points clé, il faut déterminer leur orientation en calculant le gradient dans le voisinage
du point clé. Grâce au orientation des voisins des point clé il est possible de créer un histogramme des orientation. Ainsi
l'orientation du point clé correspond aux pics les plus importants de l'histogramme.
La construction de ces points clés permets à ceux-ci d'être invariant aux changements d'échelle et aux rotations.

%TODO SHOT -> voir si on en parle
\subsubsection{Apprentissage automatique}
L'apprentissage automatique est un outil permettant à une machine de prendre des décisions rapidement.
Pour fonctionner cette outil à besoin de donnée déjà traité sur un domaine précis. Il existe de nombreuses
techniques d'apprentissage automatique. Lors de ce projets, je me suis intéressé à deux techniques en 
particulier qui sont parmis les plus utilisé dans le domaine de l'image : la \og forêt d'arbres décisionnels \fg \ et les
\og machines à vecteurs de support \fg (SVM).\\

Le principe de la forêt d'arbres décisionnels\cite{randomDecisionForest} est de créer un ensemble d'arbre.
Dans le cas de la méthode de J. Shotton et al\cite{kinectSegmentation}, chaque arbre correspond à une posture.
Les noeuds des arbres correspondent aux caractéristiques calculés. L'algorithme test des noeuds lui permettant ainsi de tracer un
chemin vers une feuille qui donne un résultat. L'arbre dont la feuille nous donne le résultat le plus proche 
de la valeur rechercher nous fournit la solution à notre problème.\\

\begin{figure}[!h]
  \begin{center}
    \includegraphics[width=7cm]{image/randomForest.png}
    \caption{Exemple d'arbre de décision}
  \end{center}
\end{figure}

Le SVM\cite{SVM} quant à lui est un ensemble de technique d'apprentissage supervisé permettant la résolution de 
problème de discrimination.
%TODO faire SVM

\subsubsection{Sac de mot (bag of word)}
Le sac de mot est une représentation dont la première utilisation était de décrire un document texte en fonction d'un dictionnaire.
Le dictionnaire est un ensemble de mot capable de décrire tous les textes.
Le principe est de compter le nombre d'occurence d'un mot dans un texte et ce pour chaque mot du dictionnaire. Cette représentation
à ensuite était utilisé dans le domaine de la vision par ordinateur\cite{BagOfWord} en remplaçant le dictionnaire de mot par un dictionnaire
de caractèristiques. Grâce à cette représentation nous obtenons un vecteur de la taille du dictionnaire composé du nombre 
d'occurence de chaque caractèristique du dictionnaire. L'intérêt de cette représentation est d'uniformiser la structure des données
afin d'avoir un vecteur de même taille pour toutes les images, mais cela nécessite une première phase de création du dictionnaire
avec l'ensemble des caractèristiques des images traitées.

\subsection{Positionnement de modèle}
%moment d'inertie pour la position des membres
%PCA
%ICP
Afin de réaliser la correspondance entre deux modèles P. J. Besl et al\cite{ICP} développe l'algorithme \og iterative closest point \fg (ICP).
Cette algorithme recherche l'ensemble des translations et rotations nécessaire à la mise en correspondance de deux modèles similaires. Pour cela,
celui-ci fonctionne en quatre étapes :
\begin{itemize}
  \item Associer les points grâce aux critères du plus proche voisin. Pour cela, il suffit de calculer la distance euclidienne d'un
   point avec tous les autres points qui font partis du balayage que nous voulons comparer et de prendre la distance la plus petite.
  \item Estimer la transformation des points grâce à une fonction d'erreur quadratique moyenne, permettant ainsi de trouver la meilleure
  transformation possible.
  \item Effectuer la transformation du nuage de points ayant la plus petite erreur.
  \item Itérer jusqu'à ce qu'on ait atteint la condition de fin fixée par l'utilisateur.
\end{itemize}

%D. L. Dinh et al\cite{PCA} utilisent 
