%\subsection{Segmentation de l'environnement intérieur}
%segmentation par l'utilisateur
%\subsection{Création d'une base de connaissance}
%utilisation opencv
%creation du dictionnaire
%calcul du descripteur des objets -> detail sur le nombre d'objet et sur le nombre d'exemplaire
%test sur une base composé de mesh et sur une autre composé de nuage de point
%\subsection{Reconnaissance des objets} 
%simple comparaison avec le descripteur
%TODO voir ce qu'on peut mettre dans cette section

\subsection{Objectif}
%interface
Le but de cette seconde application est de pouvoir synthétiser un environnement intérieur au travers d'une interface
simple et intuitive à l'aide d'une caméra. 
Pour cela, nous allons dans un premier temps récupérer un nuage de points d'une pièce intérieure. Puis à partir
de ce nuage de points, nous allons segmenter la pièce afin de pouvoir détecter des objets à l'intérieur de celle-ci.
L'utilisateur doit ensuite sélectionner un objet, ce qui déclenchera l'apparition d'une liste contenant un ensemble de modèle 3D
d'objets équivalents à celui sélectionné. L'utilisateur n'a plus qu'à choisir l'objet qui lui convient dans la liste,
afin de l'ajouter dans un autre environnement 3D présent dans l'application.\\

Lors de ce scénario, nous pouvons voir qu'il y a deux grandes étapes qu'il faudra réaliser dans l'application. Tout d'abord, il faut 
segmenter le nuage de points de la pièce, dans le but de détecter des objets. Puis il faut reconnaître les objets détectés afin
d'afficher la bonne liste d'objets 3D. Nous avons décidé, pour faciliter le développement, de laisser l'utilisateur sélectionner
l'objet qu'il souhaite dans le nuage de points, afin de ne pas avoir à développer la détection d'objet dans le nuage de points.
Pour cela, nous nous inspirons des travaux de T. Shao et al\cite{interactiveSeg} et de J. Xiao et al\cite{interactionSeg2}.
Il nous reste donc à reconnaitre l'objet sélectionné par l'utilisateur.

\subsection{Création de la base d'apprentissage}
Afin de pouvoir reconnaître un objet, nous avons besoin d'une base d'apprentissage contenant les descripteurs d'un ensemble
d'objets. Pour créer cette base, nous utilisons la représentation du \og bag of word \fg ainsi que la méthode d'apprentissage 
automatique SVM\cite{SVM} disponible dans la librairie opencv. Pour la création de notre base, nous utilisons une vingtaine
d'images de six objets différents. Les nuages de points dont nous nous sommes servis pour la création de notre base, viennent de 
K. Lai et al\cite{Base1} issue de M. Firman\cite{generalBase} qui a regroupé un ensemble de bases d'objets provenant de caméra 3D.
La base que nous utilisons est composée de plusieurs exemplaires de chaque objet, et chaque exemplaire d'objet est représenté par 
plusieurs images représentant plusieurs angles de vue de l'objet. L'acquisition des objets a été réalisée avec une caméra 
Kinect v1.\\

%Nous créons une seconde base d'apprentissage utilisant le même principe que précédemment, mais qui utilise des images que nous 
%produisons nous même avec notre caméra Kinect v2. Cette base comporte quatre objets, chacun représenté par dix à quinze angles de vue différent.
%L'intérêt de cette seconde base est de vérifier que la qualité de l'acquisition ne fait pas varier significativement les résultats de
%la reconnaissance d'objet. Cependant, le fait de créer nous même notre base ne nous permet pas d'avoir autant de donné que dans la base de 
%K. Lai et al\cite{Base1} à cause du temps que cela prend de créer une tel base.

\subsection{Reconnaissance d'objet}
Le descripteur que nous utilisons pour cette application est le descripteur FPFH\cite{FPFH} qui est l'un des plus efficaces dans la
reconnaissance d'objet à partir de nuage de points. Nous calculons ce descripteur sur l'ensemble des images de notre base afin de
créer le dictionnaire de notre \textit{bag of words}. A cette étape du développement, nous avons créé le dictionnaire, il faut
alors recalculer le descripteur sur chacun des objets afin de déterminer les caractèristiques de chaqu'un d'entre eux. Cela nous
permet d'obtenir un histogramme pour chaque objet comportant les caractèristiques qu'il contient et le nombre de fois où il
apparaît. Il ne reste plus qu'à utiliser le descripteur du \textit{bag of word} dans SVM afin de finaliser la création de 
notre base d'apprentissage. Celle-ci est donc un vecteur d'une taille égale au nombre de classes, donc six, 
composé d'histogramme dont la taille est égale au nombre de caractèristiques.\\

\begin{figure}[!ht]
  \begin{center}
    \includegraphics[height=8cm]{image/schemaBase.png}
    \caption{Schéma de la création de la base d'apprentissage}
  \end{center}
\end{figure}
%TODO parler des résultats de la reconnaissance
Une fois la base d'apprentissage créée, nous ajoutons la méthode de reconnaissance de forme dans l'application. Cette méthode est basée
sur l'approche du sac de mots. Elle permet d'apparier le modèle capturé par la caméra avec l'ensemble des modèles d'apprentissage.
D'abord, nous filtrons les données que l'utilisateur a séléctionné. Nos méthodes de sélection n'étant pas très précises, il arrive
couramment que l'utilisateur sélectionne du bruit ou des parties de l'environnement. Pour filtrer les données, nous utilisons
les deux mêmes classes que dans l'application précédente, c'est-à-dire \textit{StatisticalOutlierRemoval} et \textit{VoxelGrid}.
Lorsque le filtrage est réalisé, il nous faut calculer le même descripteur que dans notre base sur les données sélectionnées par 
l'utilisateur, puis envoyer le résultat dans notre SVM afin qu'il détermine la classe de l'objet.\\

En utilisant les objets de la base de K. Lai et al\cite{Base1} nous obtenons des résultats très insuffisant de la reconnaissance 
des objets provenant de notre caméra Kinect v2. Les différences entre nos données et celle provenant de cette base viennent
de notre sélection et de la caméra. Dans les données de la base, le bruit est presque null tandis que dans notre sélection il 
y en a un peu plus, malgrès que nous en supprimions un maximum grâce à la librairie PCL. La principal différence entre la caméra
de K. Lai et al\cite{Base1} et la notre est la résolution de la caméra qui est plus grande dans notre cas. Avec ces paramètres, ne parvient
à reconnaitre qu'un objet sur les six testés et cela dépend du nombre de mot que nous mettons dans le dictionnaire. Cette première base nous 
à permis de déterminer que le nombre de mot idéal qu'il faut mettre dans le dictionnaire pour notre application est de deux milles.\\

\subsection{Interface utilisateur}
L'interface utilisateur comprend deux fenêtres. La première comporte une vue avec le nuage de points récupéré à partir de la Kinect
et un ensemble d'options. Parmi ces options, nous avons la navigation dans le nuage de points et le choix de la technique de 
sélection. La première sélection est une simple sélection rectangulaire où l'ensemble des données à l'intérieur d'un rectangle formé
par deux points est sélectionné. La seconde est une sorte de pinceau sélectionnant les données en dessous de la souris, ainsi que celles dans 
un voisinage prédéfini autour de celle-ci. Lorsque l'utilisateur a sélectionné un objet et que l'application l'a reconnu, un nombre \textit{n} de cases 
avec des objets apparaît. Les objets dans ces cases sont les mêmes que l'objet sélectionné. Lorsque l'utilisateur clique sur l'une 
de ces cases, l'objets correspondant est ajouté dans la seconde fenêtre.
La seconde fenêtre contient l'environnement final dans lequel il y a déjà une pièce modèlisée avec des objets 3D. L'objet sélectionné est ajouté dans cette environnement. Il est ensuite possible de déplacer l'objet dans l'environement virtuel en
cliquant dessus afin de le positionner à l'endoit voulu dans la pièce.

\begin{figure}[!h]
  \begin{center}
    \includegraphics[height=7cm]{image/appliObjet.PNG}
    \caption{Interface de la seconde application}
  \end{center}
\end{figure}

\begin{figure}[!h]
  \begin{center}
    \includegraphics[height=4.5cm]{image/selection1.png}
    \includegraphics[height=4.5cm]{image/selection2.png}
    \caption{Schéma des deux type de sélection disponible dans l'apllication. Les points verts
    sont les points sélectionnés et en noire les autres.}
  \end{center}
\end{figure}

\subsection{Experimentations}
Suite à la création de notre base d'apprentissage à partir de nuages de points que nous avons récupérés dans la base de 
K. Lai et al\cite{Base1}, nous testons celle-ci afin de valider notre méthode. 
Pour nos tests, nous récupérons dix modèles pour chaque classe de notre base. Ces modèles viennent de la base de 
K. Lai et al\cite{Base1}, mais ne font pas partie de celle que nous avons créée. Nous testons notre algorithme avec ces 
modèles et nous expérimentons également différentes tailles de dictionnaires.\\
\begin{figure}[!ht]
  \begin{center}
    \begin{tabular}{|c|c|c|c|c|c|c|}
    \hline
    \backslashbox{Testé}{Résultat} & balle & banane & bouteille & téléphone portable & clavier & mug\\
    \hline
    balle & 10 & 0 & 0 & 0 & 0 & 0\\
    \hline
    banane & 0 & 7 & 0 & 2 & 0 & 1\\
    \hline
    bouteille & 0 & 6 & 2 & 0 & 0 & 2\\
    \hline
    téléphone portable & 0 & 1 & 0 & 7 & 2 & 0\\
    \hline
    clavier & 0 & 2 & 0 & 8 & 0 & 0\\
    \hline
    mug & 0 & 0 & 0 & 0 & 0 & 10\\
    \hline
    \end{tabular}
    \caption{Résultats obtenus avec un dictionnaire de mille mots}
    \label{tab:result1000}
  \end{center}
\end{figure}

\begin{figure}[!ht]
  \begin{center}
    \begin{tabular}{|c|c|c|c|c|c|c|}
    \hline
    \backslashbox{Testé}{Résultat} & balle & banane & bouteille & téléphone portable & clavier & mug\\
    \hline
    balle & 10 & 0 & 0 & 0 & 0 & 0\\
    \hline
    banane & 4 & 5 & 0 & 0 & 0 & 1\\
    \hline
    bouteille & 1 & 5 & 3 & 0 & 0 & 1\\
    \hline
    téléphone portable & 0 & 0 & 0 & 6 & 4 & 0\\
    \hline
    clavier & 0 & 3 & 0 & 5 & 2 & 0\\
    \hline
    mug & 1 & 0 & 0 & 0 & 0 & 9\\
    \hline
    \end{tabular}
    \caption{Résultats obtenus avec un dictionnaire de deux milles mots}
    \label{tab:result2000}
  \end{center}
\end{figure}

Comme nous pouvons le voir sur les Fig. \ref{tab:result1000} et \ref{tab:result2000} les résultats ne sont pas très 
satisfaisants. Quelque soit la taille du dictionnaire, nous arrivons à un résultat qui atteint approximativement 60\%
de reconnaissance. Certains objets sont parfaitement reconnus, comme la balle ou le mug, d'autres objets ne sont pas du tout 
reconnus comme la bouteille. Nous n'avons pas eu le temps, durant ce stage, de tester davantage de cas de figure en modifiant les différents 
paramètres de l'application, ce qui peut expliquer ces résultats assez faibles. Nous nous sommes concentrés plutôt sur la taille
du dictionnaire, mais d'autres paramètres peuvent encore améliorer les résultats.

\subsection{Travaux futurs et applications}
L'application finale de ce projet est assez complète et très fonctionnelle, cependant il existe plusieurs extensions qu'il 
est possible d'ajouter au projet. Par exemple, notre liste d'objets proposés à l'utilisateur pour ajouter à la 
scène 3D n'est pas assez conséquente. Nous pourrions ajouter plus d'objets en récupérant une base venant de grands magasins
de meubles par exemple. Ainsi l'utilisateur pourrait potentiellement retrouver le meuble qu'il a scanné. Cela pourrait
déboucher sur une application beaucoup plus importante, où l'environnement dans lequel nous souhaitons ajouter des meubles
représente la future maison d'une personne. Si cette personne souhaite voir comment elle pourrait agencer ses meubles dans 
son prochain investissement, elle n'aurait qu'à récupérer un modèle 3D des lieux et scanner l'ensemble de ses meubles pour
les ajouter dans la scène.
